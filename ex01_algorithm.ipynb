{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLE: Assignment 1\n",
    "## A-Priori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = \"data/conditions.parquet\"\n",
    "RESULTS_DIRECTORY_PATH = \"data/\"\n",
    "\n",
    "MIN_SUPPORT_THRESHOLD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 21:36:17 WARN Utils: Your hostname, omen resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface wlo1)\n",
      "24/04/03 21:36:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/03 21:36:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"A-Priori\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             PATIENT|          CONDITIONS|\n",
      "+--------------------+--------------------+\n",
      "|0000055d-e9a9-4f6...|[65966004, 10509002]|\n",
      "|0000e9ce-2e20-4c2...|[65966004, 161140...|\n",
      "|0000fc30-1096-40b...|[271737000, 59621...|\n",
      "|0001b288-1320-470...|[162864005, 72892...|\n",
      "|000246a4-c6f5-480...|[65363002, 284549...|\n",
      "|0003a636-b172-48c...|[196416002, 62106...|\n",
      "|0006d39d-364a-46a...|[428251008, 59621...|\n",
      "|0007a215-694b-428...|[162864005, 72892...|\n",
      "|00085029-7bdd-467...|[271737000, 53741...|\n",
      "|0008dd63-85c3-47b...|[58150001, 271737...|\n",
      "|0008ed08-1899-444...|[162864005, 10509...|\n",
      "|00091bb6-7352-43b...|[53741008, 105090...|\n",
      "|000a949e-82d6-441...|[271737000, 44481...|\n",
      "|000b05e4-c63c-40c...|[162864005, 27173...|\n",
      "|000c905e-46d1-4d4...|[43878008, 368581...|\n",
      "|000da8dd-2917-4bd...|[271737000, 59621...|\n",
      "|000e6ebf-8ad3-430...|[15777000, 558220...|\n",
      "|000eb281-9fa1-446...|[162864005, 72892...|\n",
      "|00106d6a-f7b9-455...|[162864005, 36971...|\n",
      "|0011b210-c80b-4ed...|[241929008, 43878...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .load(\"data/conditions.parquet\")\n",
    "\n",
    "baskets = data.rdd\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from time import perf_counter\n",
    "\n",
    "def a_priori(baskets_rdd, min_support_threshold, max_k):\n",
    "    \"\"\"\n",
    "        Compute frequent itemsets using the A-Priori algorithm.\n",
    "\n",
    "        args:\n",
    "            baskets_rdd: RDD of baskets (lists of items).\n",
    "            min_support_threshold: minimum count of an itemset to be considered frequent.\n",
    "            max_k: maximum size of the itemsets to compute.\n",
    "    \"\"\"\n",
    "\n",
    "    # First pass: compute frequent itemsets of size 1\n",
    "    t0 = perf_counter()\n",
    "\n",
    "    frequent_k_items = baskets_rdd.flatMap(\n",
    "        lambda basket: [(item, 1) for item in basket.CONDITIONS]\n",
    "    ) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .filter(lambda entry: entry[1] >= min_support_threshold) \\\n",
    "    .map(lambda entry: entry[0]) \\\n",
    "    .collect()\n",
    "\n",
    "    frequent_k_items = sc.broadcast(set(frequent_k_items))\n",
    "\n",
    "    print(f\"Computed frequent itemsets of size 1 in {perf_counter() - t0} seconds\")\n",
    "\n",
    "    if max_k == 1:\n",
    "        return frequent_k_items.value\n",
    "\n",
    "    k = sc.broadcast(2)\n",
    "\n",
    "    while True:\n",
    "        if k.value == 2: # k = 2\n",
    "            build_candidates = lambda basket: [((item1, item2), 1) for item1 in basket.CONDITIONS for item2 in basket.CONDITIONS\n",
    "            if item1 < item2 and item1 in frequent_k_items.value and item2 in frequent_k_items.value]\n",
    "        else: # k > 2\n",
    "            build_candidates = lambda basket: [\n",
    "                (itemset, 1) for itemset in combinations(sorted(basket.CONDITIONS), k.value)\n",
    "                if all(subset in frequent_k_items.value for subset in combinations(itemset, k.value - 1))\n",
    "            ]\n",
    "\n",
    "        t0 = perf_counter()\n",
    "        frequent_k_items = baskets_rdd.flatMap(build_candidates) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .filter(lambda entry: entry[1] >= min_support_threshold) \\\n",
    "        \n",
    "        if k.value == max_k:\n",
    "            res = frequent_k_items.sortBy(lambda entry: entry[1], ascending=False).collect()\n",
    "            print(f\"Computed frequent itemsets of size {k.value} in {perf_counter() - t0} seconds\")\n",
    "            return res\n",
    "\n",
    "        frequent_k_items = sc.broadcast(set(frequent_k_items.map(lambda entry: entry[0]).collect()))\n",
    "        print(f\"Computed frequent itemsets of size {k.value} in {perf_counter() - t0} seconds\")\n",
    "\n",
    "        k = sc.broadcast(k.value + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed frequent itemsets of size 1 in 10.10022094699525 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed frequent itemsets of size 2 in 15.236589359999925 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('195662009', '444814009'), 343651),\n",
       " (('10509002', '444814009'), 302516),\n",
       " (('15777000', '271737000'), 289176),\n",
       " (('162864005', '444814009'), 243812),\n",
       " (('271737000', '444814009'), 236847),\n",
       " (('15777000', '444814009'), 236320),\n",
       " (('10509002', '195662009'), 211065),\n",
       " (('444814009', '59621000'), 203450),\n",
       " (('162864005', '195662009'), 167438),\n",
       " (('40055000', '444814009'), 165530)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_2_itemsets = a_priori(baskets, MIN_SUPPORT_THRESHOLD, 2)\n",
    "frequent_2_itemsets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed frequent itemsets of size 1 in 6.687722422000661 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed frequent itemsets of size 2 in 14.630646610996337 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed frequent itemsets of size 3 in 41.26767710100103 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('15777000', '271737000', '444814009'), 192819),\n",
       " (('10509002', '195662009', '444814009'), 139174),\n",
       " (('15777000', '195662009', '271737000'), 132583),\n",
       " (('10509002', '15777000', '271737000'), 115510),\n",
       " (('162864005', '195662009', '444814009'), 111860),\n",
       " (('195662009', '271737000', '444814009'), 108560),\n",
       " (('15777000', '195662009', '444814009'), 108083),\n",
       " (('15777000', '271737000', '59621000'), 99818),\n",
       " (('10509002', '162864005', '444814009'), 97384),\n",
       " (('10509002', '271737000', '444814009'), 94793)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_3_itemsets = a_priori(baskets, MIN_SUPPORT_THRESHOLD, 3)\n",
    "frequent_3_itemsets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(RESULTS_DIRECTORY_PATH + \"frequent_2_itemsets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(frequent_2_itemsets, f)\n",
    "\n",
    "with open(RESULTS_DIRECTORY_PATH + \"frequent_3_itemsets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(frequent_3_itemsets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1157578"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total = baskets.count()\n",
    "n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics (support1, support2, support_union):\n",
    "    p1 = support1 / n_total\n",
    "    p2 = support2 / n_total\n",
    "\n",
    "    confidence = support_union / support1\n",
    "    interest = confidence - p2\n",
    "    lift = confidence / p2\n",
    "    \n",
    "    x = max(p1 + p2 - 1, 1/n_total) / (p1 * p2)\n",
    "    standardised_lift = (lift - x) / ((1/max(p1, p2)) - x)\n",
    "\n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"interest\": interest,\n",
    "        \"lift\": lift,\n",
    "        \"standardised_lift\": standardised_lift\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frequent_1_itemsets = baskets.flatMap(\n",
    "    lambda basket: [(item, 1) for item in basket.CONDITIONS]\n",
    ") \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".filter(lambda entry: entry[1] >= MIN_SUPPORT_THRESHOLD) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_1_itemsets_dict = dict(frequent_1_itemsets)\n",
    "rules = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations X -> Y with a standardised lift > 0.2\n",
    "for (item1, item2), support_union in frequent_2_itemsets:\n",
    "    support1 = frequent_1_itemsets_dict[item1]\n",
    "    support2 = frequent_1_itemsets_dict[item2]\n",
    "    \n",
    "    # evaluate item1 -> item2 (I = {item1}, j = {item2})\n",
    "    metrics1 = metrics(support1, support2, support_union)\n",
    "\n",
    "    if metrics1[\"standardised_lift\"] >= 0.2:\n",
    "        rules.append({\"antecedent\": int(item1), \"consequent\": int(item2), **metrics1})\n",
    "\n",
    "    # evaluate item2 -> item1 (I = {item2}, j = {item1})\n",
    "    metrics2 = metrics(support2, support1, support_union)\n",
    "    if metrics2[\"standardised_lift\"] >= 0.2:\n",
    "        rules.append({\"antecedent\": int(item2), \"consequent\": int(item1), **metrics2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_2_itemsets_dict = dict(frequent_2_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations (X, Y) -> Z with a standardised lift > 0.2\n",
    "for itemset, support_union in frequent_3_itemsets:\n",
    "    for j in itemset:\n",
    "        I = tuple(item for item in itemset if item != j)\n",
    "\n",
    "        support1 = frequent_2_itemsets_dict[I]\n",
    "        support2 = frequent_1_itemsets_dict[j]\n",
    "\n",
    "        metrics_ = metrics(support1, support2, support_union)\n",
    "\n",
    "        if metrics_[\"standardised_lift\"] >= 0.2:\n",
    "            rules.append({\"antecedent\": tuple(sorted(I, key=lambda x: int(x))), \"consequent\": int(j), **metrics_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25665"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rules_df = pd.DataFrame(rules)\n",
    "rules_df.sort_values(\"standardised_lift\", ascending=False, inplace=True)\n",
    "\n",
    "rules_df.to_string(RESULTS_DIRECTORY_PATH + \"association_rules.txt\", index=False, float_format=lambda x: f\"{x:.15f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
