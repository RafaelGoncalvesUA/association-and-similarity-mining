{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLE Assignment 1\n",
    "## 2. Locality Sensitive Hashing\n",
    "Students were provided with `covid_news_full.json.bz2` (60357 articles) and `covid_news_small.json.bz2` (18898 articles) files. The task was to implement Locality Sensitive Hashing (LSH) to find similar news articles.\n",
    "</br></br>\n",
    "For the following code we have two auxiliary files:\n",
    "- `lsh_utils.py` which contains the implementation of the functions used by pyspark to implement LSH.\n",
    "- `constants.py` which contains the constants used in this notebook and the grid search results for the best parameters for the LSH algorithm. It is possible to run the `constants.py` file himself to get the best parameters for the LSH algorithm and to get the best parameters shown in a graph, as shown in `best_b_and_r_expected.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 11)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pyspark\n",
    "from itertools import combinations\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import lsh_utils as lsh\n",
    "importlib.reload(lsh)\n",
    "\n",
    "from constants import *\n",
    "\n",
    "print(BEST_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 22:06:34 WARN Utils: Your hostname, diogo-VivoBook resolves to a loopback address: 127.0.1.1; using 192.168.1.236 instead (on interface wlp1s0)\n",
      "24/04/05 22:06:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/05 22:06:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"15g\")\n",
    "conf.set(\"spark.driver.memory\", \"15g\")\n",
    "\n",
    "spark = pyspark.SparkContext(appName=\"Locality Sensitive Hashing\", conf=conf).getOrCreate()\n",
    "\n",
    "# spark = pyspark.SparkContext(appName=\"Locality Sensitive Hashing\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "The number of bands and rows should be parameters. Select a combination that finds as\n",
    "candidates at least 90% of pairs with 85% similarity and less than 5% of pairs with 60% similarity.\n",
    "</br></br>\n",
    "The request of the 2.1 question was to implement the LSH algorithm using a combination of parameters that finds as candidates with specific similarity values. For that we had to do some hyperparameter tuning to find the best combination of parameters before executing the algorithm.\n",
    "After analysing the hyperparameters, using the grid search present in `constants.py`, we have chosen the values of `b=13` and `r=11` since they perform well, meet the requirements requested in the assignment and are the combination of values that will generate the least number of hash functions, improving the performance of the algorithm. There are hyperparameters that perform better, but they generate a large number of hash functions, which can impair the performance of the algorithm.\n",
    "</br></br>\n",
    "Some more constants were necessary for the implementation of the LSH algorithm and this ones were thinked and defined by hand. The constants are:\n",
    "- `SHINGLE_SIZE` which is the size of the shingles that will be used to represent the articles. We have chosen the value of 9, since a lower value would generate a lot of collisions, the value is the mostly used in practice for this kind of problem and since higher values would not have a significant impact on the algorithm's performance.\n",
    "- `BUCKET_SIZE` which is the size of the buckets that will be used to check if there are collisions in the hash functions generated by the LSH algorithm. We have chosen the value of 100000, since it does not impact the algorithm's time complexity and high values have great impact on the algorithm's performance since wrong collisions will not be as much generated.\n",
    "- `N_SHINGLES` which was used in the minhash algorithm to generate the signatures of the articles. This is not the real number of shingles generated by the documents, but it's a number that should be greater than it. We have chosen a value of 2**32, since the shingles are converted to their hash representation and this value is the maximum value that can be represented by a 32-bit integer.\n",
    "- `PRIME` which is the prime number used in the hash functions generated by the LSH algorithm. Since it had, ideally, to be higher than the number of shingles generated by the documents, we have chosen the next prime number after the value of `N_SHINGLES`, which is 4294967311.\n",
    "- `AB` which is a list of random values used in the hash functions generated by the LSH algorithm. They are generated in execution time, using a custom seed, but are stored in the variables, since it is not suposed to change during the execution of the algorithm and they might be needed in the future for the addition of new documents.\n",
    "</br></br>\n",
    "As shown in the next cells, our LSH algorithm consisted of the following steps:\n",
    "1. Read the data from the file and retrieve the shingles from the articles. We made some text processing to remove ponctuation and convert the text to lowercase.\n",
    "2. Convert from shingles to a signature representation generated using the minhash algorithm.\n",
    "3. Convert from signatures to buckets and store them for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Shingling\n",
    "Read the data from the file and retrieve the shingles from the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json file and store as tuple\n",
    "data_shingles = spark.textFile(INPUT_FILE).map(json.loads)\\\n",
    "                .map(lsh.shingles).cache()\n",
    "                # 1 - Read the file\n",
    "                # 2 - parse it to json\n",
    "                # 3 - Convert the json to tuples and convert the text to shingles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures\n",
    "Convert from shingles to a signature representation generated using the minhash algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get signatures\n",
    "data_signatures = data_shingles.map(lsh.min_hash)\n",
    "                    # 1 - Convert the shingles to minhash signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH\n",
    "Convert from signatures to buckets and store them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split the signatures into N_ROWS elements, put into buckets\n",
    "data_buckets = data_signatures.map(lsh.signatures_to_buckets)\\\n",
    "                .flatMap(lambda x: [((bucket, i), [x[0]]) for i, bucket in enumerate(x[1])])\\\n",
    "                .reduceByKey(lambda x, y: x + y)\n",
    "                # 1 - Convert the signatures to buckets\n",
    "                # 2 - Split the signatures keeping the bucket number (bucket, index of signature)\n",
    "                # 3 - Join the ids by bucket and index\n",
    "\n",
    "# We now have a list of buckets with the ids of the signatures that belong to it, if a bucket contains more than one id, it means that those are candidate pairs\n",
    "\n",
    "# delete the output folder if it exists\n",
    "if os.path.exists(OUTPUT_FOLDER):\n",
    "    shutil.rmtree(OUTPUT_FOLDER)\n",
    "\n",
    "# Save the buckets to a file\n",
    "data_buckets.saveAsTextFile(OUTPUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output folder present in the ex2, represent the output of the LSH algorithm using the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "Implement a function that, given a news article, returns all other news articles that are at least\n",
    "85% similar. You should make use of a pre-processed set of candidate pairs, obtained by LSH,\n",
    "and return only the ones that have Jaccard similarity–considering the shingles–above 85%.\n",
    "</br></br>\n",
    "For this question we had to implement a function that, given a news article, returns all other news articles that are at least 85% similar using the Jacard similarity.\n",
    "To get the candidates, we used the previous processed buckets, stored in the `OUTPUT_FOLDER`, got the distinct candidate pairs, since even if multiple buckets were similar we only wanted to check the similarity between the articles once, generated the list of unique candidates, and then we calculated the Jacard similarity between the articles parallelizing the process using pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the lsh\n",
    "data_buckets = spark.textFile(OUTPUT_FOLDER).map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar article 0: ('1349669108229533696', '1349654208337862656', 1.0)\n",
      "Similar article 1: ('1349669108229533696', '1349454372560920578', 1.0)\n",
      "Similar article 2: ('1349669108229533696', '1349823098623819784', 0.9942122186495177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def similar_articles(doc_id, expected_similatiry):\n",
    "    # If the program runs all at once, the data_shingles will be cached and it will be faster to lookup the shingles\n",
    "    doc_shingles = data_shingles.lookup(doc_id)[0]\n",
    "\n",
    "    # Candidate pairs of a specific document\n",
    "    candidate_pairs = data_buckets.filter(lambda x: len(x[1]) > 1)\\\n",
    "                            .map(lsh.get_candidates)\\\n",
    "                            .flatMap(lambda x: x)\\\n",
    "                            .map(lambda x: (x[0], x[1]) if x[0] < x[1] else (x[1], x[0]))\\\n",
    "                            .distinct()\\\n",
    "                            .filter(lambda x: doc_id in x)\n",
    "                            # 1 - Get the buckets with more than 1 element\n",
    "                            # 2 - Get the candidates (pairs of similar documents)\n",
    "                            # 3 - Flatten the list of candidates\n",
    "                            # 4 - Sort the candidates to avoid duplicates in different order\n",
    "                            # 5 - Remove duplicates\n",
    "                            # 6 - Filter the candidates that contain the doc_id\n",
    "\n",
    "    # List of IDs of the candidates that should be compared to the specific document\n",
    "    candidates = candidate_pairs.flatMap(lambda x: [x[0], x[1]])\\\n",
    "                                .distinct()\\\n",
    "                                .filter(lambda x: x != doc_id)\\\n",
    "                                .collect()\n",
    "\n",
    "    return spark.parallelize([(doc_id, x[0], lsh.jaccard_similarity(doc_shingles, x[1])) for _, x in enumerate(data_shingles.filter(lambda x: x[0] in candidates).collect())]).filter(lambda x: x[2] >= expected_similatiry)\n",
    "\n",
    "\n",
    "doc_similar_articles = similar_articles('1349669108229533696', 0.85)\n",
    "                              \n",
    "for i, occurence in enumerate(doc_similar_articles.collect()):\n",
    "    print(f\"Similar article {i}: {occurence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "Using a sample of the dataset, evaluate the LSH method by calculating the Jaccard similarities\n",
    "and obtaining the percentage of false positives and false negatives.\n",
    ">**Note**: We average over multiple samples to get more robust values.\n",
    "\n",
    "\n",
    "For this question we had to evaluate the LSH method by calculating the Jacard similarities and obtaining the percentage of false positives and false negatives. For this we got a sample of the dataset, generated all the combinations of articles in that sample, and calculated the Jacard similarity between them using the shingles. It is important to refer that we considered a document to be similar to another if the Jacard similarity was greater than 85% and not similar otherwise. After getting the similarities between documents, we checked if the same classification was made by the LSH algorithm, and calculated the percentage of false positives and false negatives. All this was done using pyspark to parallelize the process and speed up the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 22:38:11 WARN TaskSetManager: Stage 17 contains a task of very large size (1546 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 17:=====================================================>(143 + 1) / 144]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives: 0.03%\n",
      "False negatives: 0.0%\n",
      "\n",
      "True Positives: 1779\n",
      "False Negatives: 0\n",
      "False Positives: 133\n",
      "True Negatives: 497588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# For the following code, we assume that if the similarity is > 85% the document is positive and <= 85% the document is negative\n",
    "\n",
    "# Calculate the false positives, false negatives, true positives and true negatives\n",
    "def process_combination(comb):\n",
    "    shingle1, shingle2 = comb\n",
    "    if shingle1[0] < shingle2[0]:\n",
    "        comb = (shingle1, shingle2)\n",
    "    else:\n",
    "        comb = (shingle2, shingle1)\n",
    "\n",
    "    similarity = lsh.jaccard_similarity(comb[0][1], comb[1][1])\n",
    "\n",
    "    if similarity > 0.85:\n",
    "        if (comb[0][0], comb[1][0]) in candidate_pairs.value:\n",
    "            true_positives.add(1)\n",
    "        else:\n",
    "            false_negatives.add(1)\n",
    "    else:\n",
    "        if (comb[0][0], comb[1][0]) in candidate_pairs.value:\n",
    "            false_positives.add(1)\n",
    "        else:\n",
    "            true_negatives.add(1)\n",
    "            \n",
    "# Initialize paralell counters\n",
    "false_positives = spark.accumulator(0)\n",
    "false_negatives = spark.accumulator(0)\n",
    "true_positives = spark.accumulator(0)\n",
    "true_negatives = spark.accumulator(0)\n",
    "\n",
    "# Get a x sample of data\n",
    "data_quant = 1000\n",
    "data_sample = data_shingles.takeSample(withReplacement=False, num=data_quant, seed=0)\n",
    "\n",
    "# Get the ids of the sample\n",
    "data_sample_ids = set([x[0] for x in data_sample])\n",
    "\n",
    "# Create a RDD with the sample\n",
    "data_sample = spark.parallelize(data_sample)\n",
    "\n",
    "# Get the candidate pairs\n",
    "candidate_pairs = set(data_buckets.filter(lambda x: len(x[1]) > 1)\\\n",
    "                    .map(lsh.get_candidates)\\\n",
    "                    .flatMap(lambda x: x)\\\n",
    "                    .map(lambda x: (x[0], x[1]) if x[0] < x[1] else (x[1], x[0]))\\\n",
    "                    .distinct()\\\n",
    "                    .filter(lambda x: x[0] in data_sample_ids and x[1] in data_sample_ids)\\\n",
    "                    .collect())\n",
    "\n",
    "# Broadcast the candidate pairs to all nodes\n",
    "candidate_pairs = spark.broadcast(candidate_pairs)\n",
    "\n",
    "# Generate combinations and process each\n",
    "data_combinations = data_sample.cartesian(data_sample).filter(lambda x: x[0][0] < x[1][0])\n",
    "data_combinations.foreach(process_combination)\n",
    "\n",
    "# Calculate the percentages of false positives and false negatives\n",
    "false_positives_percentage = false_positives.value / (false_positives.value + true_negatives.value)\n",
    "false_negatives_percentage = false_negatives.value / (false_negatives.value + true_positives.value)\n",
    "        \n",
    "print(f\"False positives: {round(false_positives_percentage*100, 2)}%\")\n",
    "print(f\"False negatives: {round(false_negatives_percentage*100, 2)}%\")\n",
    "\n",
    "print()\n",
    "print(\"True Positives:\", true_positives.value)\n",
    "print(\"False Negatives:\", false_negatives.value)\n",
    "print(\"False Positives:\", false_positives.value)\n",
    "print(\"True Negatives:\", true_negatives.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
