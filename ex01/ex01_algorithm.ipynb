{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLE: Assignment 1\n",
    "## 1. A-Priori algorithm\n",
    "\n",
    "As explained in the first notebook [(ex01_preprocessing.ipynb)](ex01_preprocessing.ipynb), students were provided with the file `conditions.csv.gz` which lists conditions for a large set of patients. Our purpose is to find associations between conditions.\n",
    "\n",
    "To accomplish our goal, we implement the A-Priori algorithm, a classic approach for [frequent itemset mining (exercises 1.1 and 1.2)](ex01_algorithm.ipynb#1.1) and posterior [association rule learning (exercise 1.3)](./#1.3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = \"data/conditions.parquet\"\n",
    "RESULTS_DIRECTORY_PATH = \"data/\"\n",
    "\n",
    "MIN_SUPPORT_THRESHOLD = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession` offers a very simple way to read compressed files into a DataFrame. However, in order to implement the algorithm, as we intend to use RDDs, we leverage the `SparkContext` that can be accessed through the session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 13:24:13 WARN Utils: Your hostname, omen resolves to a loopback address: 127.0.1.1; using 192.168.37.0 instead (on interface wlo1)\n",
      "24/04/04 13:24:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/04 13:24:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"A-Priori\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             PATIENT|          CONDITIONS|\n",
      "+--------------------+--------------------+\n",
      "|0000055d-e9a9-4f6...|[65966004, 10509002]|\n",
      "|0000e9ce-2e20-4c2...|[65966004, 161140...|\n",
      "|0000fc30-1096-40b...|[271737000, 59621...|\n",
      "|0001b288-1320-470...|[162864005, 72892...|\n",
      "|000246a4-c6f5-480...|[65363002, 284549...|\n",
      "|0003a636-b172-48c...|[196416002, 62106...|\n",
      "|0006d39d-364a-46a...|[428251008, 59621...|\n",
      "|0007a215-694b-428...|[162864005, 72892...|\n",
      "|00085029-7bdd-467...|[271737000, 53741...|\n",
      "|0008dd63-85c3-47b...|[58150001, 271737...|\n",
      "|0008ed08-1899-444...|[162864005, 10509...|\n",
      "|00091bb6-7352-43b...|[53741008, 105090...|\n",
      "|000a949e-82d6-441...|[271737000, 44481...|\n",
      "|000b05e4-c63c-40c...|[162864005, 27173...|\n",
      "|000c905e-46d1-4d4...|[43878008, 368581...|\n",
      "|000da8dd-2917-4bd...|[271737000, 59621...|\n",
      "|000e6ebf-8ad3-430...|[15777000, 558220...|\n",
      "|000eb281-9fa1-446...|[162864005, 72892...|\n",
      "|00106d6a-f7b9-455...|[162864005, 36971...|\n",
      "|0011b210-c80b-4ed...|[241929008, 43878...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .load(\"data/conditions.parquet\")\n",
    "\n",
    "baskets = data.rdd\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.\n",
    "In each iteration of frequent itemset mining, the algorithm traverses through the dataset to determine the number of baskets that contain a particular itemset. We start with single items (k = 1) in the first iteration and proceed to 2-itemsets (k = 2), 3-itemsets (k = 3), and so on.\n",
    "\n",
    "The search space is pruned by removing itemsets that do not meet a minimum support threshold, and the most frequent items are carried forward from one iteration to the next, ideally until no more frequent itemsets can be found.\n",
    ">For this assignment, we stop at k=2 and k=3.\n",
    "\n",
    "We collect the most frequent items (l1), 2-itemsets (l2), and 3-itemsets (l3) - generically referred to as `lk` - in the form of an RDD to later extract the top 10 most frequent itemsets for K=2, K=3 [(exercise 1.2)](#1.2) or any user-defined K as `max_k`. Nevertheless, even if we call the function with `max_k=3`, we still need the most frequent items and 2-itemsets to generate association rules, so we store them as dictionaries `l1.collectAsMap()` and `l2.collectAsMap()` in the array `freq`.\n",
    "\n",
    "Given the fact that the whole process is supposed to be executed in a parallelised manner, the dictionary `freq` is **broadcasted to all nodes** to avoid shuffling the data across the network, i.e, to eliminate the overhead of sending the data to the nodes where the tasks are executed. The dictionary could not be partitioned, because all nodes must possess all entries to perform lookup operations.\n",
    "\n",
    "**Main methods**:\n",
    "- `flatMap` generates new entries (itemset, 1) for each k-itemset in a given basket (the number of rows in the RDD is increased).\n",
    "- `reduceByKey` counts the number of baskets that contain each itemset.\n",
    "- `filter` removes itemsets that do not meet the minimum support threshold.\n",
    "- `collectAsMap` stores the RDD entries in the form of a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def a_priori(baskets_rdd, min_support_threshold, max_k):\n",
    "    \"\"\"\n",
    "        Compute frequent itemsets using the A-Priori algorithm.\n",
    "\n",
    "        args:\n",
    "            baskets_rdd: RDD of baskets (lists of items).\n",
    "            min_support_threshold: minimum count of an itemset to be considered frequent.\n",
    "            max_k: maximum size of the itemsets to compute.\n",
    "    \"\"\"\n",
    "\n",
    "    # First pass: compute frequent itemsets of size 1\n",
    "    # lk denotes the frequent itemsets of size k\n",
    "    lk = baskets_rdd.flatMap(\n",
    "        lambda basket: [(item, 1) for item in basket.CONDITIONS]\n",
    "    ) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .filter(lambda entry: entry[1] >= min_support_threshold) \\\n",
    "\n",
    "    freq = [sc.broadcast(lk.collectAsMap())]\n",
    "\n",
    "    k = sc.broadcast(2)\n",
    "\n",
    "    while k.value <= max_k:\n",
    "        print(f\"Computing frequent itemsets of size {k.value}...\")\n",
    "        if k.value == 2: # k = 2\n",
    "            build_candidates = lambda basket: [((item1, item2), 1) for item1 in basket.CONDITIONS for item2 in basket.CONDITIONS\n",
    "            if item1 < item2 and item1 in freq[-1].value and item2 in freq[-1].value]\n",
    "        \n",
    "        else: # k > 2\n",
    "            build_candidates = lambda basket: [\n",
    "                (itemset, 1) for itemset in combinations(sorted(basket.CONDITIONS), k.value)\n",
    "                if all(subset in freq[-1].value for subset in combinations(itemset, k.value - 1))\n",
    "            ]\n",
    "\n",
    "        lk = baskets_rdd.flatMap(build_candidates) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .filter(lambda entry: entry[1] >= min_support_threshold) \\\n",
    "\n",
    "        freq.append(sc.broadcast(lk.collectAsMap()))\n",
    "\n",
    "        k = sc.broadcast(k.value + 1)\n",
    "\n",
    "    return lk, freq[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.\n",
    "#### Most frequent 2-itemsets (K=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing frequent itemsets of size 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('195662009', '444814009'), 343651),\n",
       " (('10509002', '444814009'), 302516),\n",
       " (('15777000', '271737000'), 289176),\n",
       " (('162864005', '444814009'), 243812),\n",
       " (('271737000', '444814009'), 236847),\n",
       " (('15777000', '444814009'), 236320),\n",
       " (('10509002', '195662009'), 211065),\n",
       " (('444814009', '59621000'), 203450),\n",
       " (('162864005', '195662009'), 167438),\n",
       " (('40055000', '444814009'), 165530)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2, _ = a_priori(baskets, MIN_SUPPORT_THRESHOLD, 2)\n",
    "l2.takeOrdered(10, key=lambda entry: -entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent 3-itemsets (K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing frequent itemsets of size 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing frequent itemsets of size 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('15777000', '271737000', '444814009'), 192819),\n",
       " (('10509002', '195662009', '444814009'), 139174),\n",
       " (('15777000', '195662009', '271737000'), 132583),\n",
       " (('10509002', '15777000', '271737000'), 115510),\n",
       " (('162864005', '195662009', '444814009'), 111860),\n",
       " (('195662009', '271737000', '444814009'), 108560),\n",
       " (('15777000', '195662009', '444814009'), 108083),\n",
       " (('15777000', '271737000', '59621000'), 99818),\n",
       " (('10509002', '162864005', '444814009'), 97384),\n",
       " (('10509002', '271737000', '444814009'), 94793)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3, freq = a_priori(baskets, MIN_SUPPORT_THRESHOLD, 3)\n",
    "l3.takeOrdered(10, key=lambda entry: -entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.\n",
    "Harnessing the broadcasted dictionary `freq` and the method `flatMap` once again, we can parallelise the generation of association rules. We were interested in rules of the forms (X) → Y and (X, Y) → Z with a minimum standardised lift of 0.2.\n",
    "\n",
    "For the first ones, we look for the most frequent pairs (A, B) and check the standardised lift of A → B and B → A. For the second ones, we look for the most frequent triples (A, B, C) and check the standardised lift of (A, B) → C, (A, C) → B, and (B, C) → A (for each item j, I \\ {j} → j). Furthermore, we also compute other metrics: lift, confidence, and interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metrics require the total number of baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1157578"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total = baskets.count()\n",
    "n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics (support1, support2, support_union):\n",
    "    p1 = support1 / n_total\n",
    "    p2 = support2 / n_total\n",
    "\n",
    "    confidence = support_union / support1\n",
    "    interest = confidence - p2\n",
    "    lift = confidence / p2\n",
    "    \n",
    "    x = max(p1 + p2 - 1, 1/n_total) / (p1 * p2)\n",
    "    standardised_lift = (lift - x) / ((1/max(p1, p2)) - x)\n",
    "\n",
    "    return (standardised_lift, lift, interest, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules X → Y (standardised lift >= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1_to_1_rule(entry):\n",
    "    (item1, item2), support_union = entry\n",
    "\n",
    "    rules = []\n",
    "\n",
    "    support1 = freq[0].value[item1]\n",
    "    support2 = freq[0].value[item2]\n",
    "\n",
    "    # evaluate item1 -> item2 (I = {item1}, j = {item2})\n",
    "    metrics1 = metrics(support1, support2, support_union)\n",
    "\n",
    "    if metrics1[0] >= 0.2: # standardised lift > 0.2\n",
    "        rules.append((item1, item2, *metrics1))\n",
    "\n",
    "    # evaluate item2 -> item1 (I = {item2}, j = {item1})\n",
    "    metrics2 = metrics(support2, support1, support_union)\n",
    "    if metrics2[0] >= 0.2: # standardised lift > 0.2\n",
    "        rules.append((item2, item1, *metrics2))\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rules = l2.flatMap(build_1_to_1_rule).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules (X, Y) → Z (standardised lift >= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations (X, Y) -> Z with a standardised lift > 0.2\n",
    "def build_2_to_1_rule(entry):\n",
    "    itemset, support_union = entry\n",
    "    rules = []\n",
    "\n",
    "    for j in itemset:\n",
    "        I = tuple(item for item in itemset if item != j)\n",
    "\n",
    "        support1 = freq[1].value[I]\n",
    "        support2 = freq[0].value[j]\n",
    "\n",
    "        metrics_ = metrics(support1, support2, support_union)\n",
    "\n",
    "        if metrics_[0] >= 0.2: # standardised lift > 0.2\n",
    "            rules.append((I, j, *metrics_))\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rules += l3.flatMap(build_2_to_1_rule).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25665"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting results with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports Pandas dataframes that can be imported as `pyspark.pandas.frame.DataFrame`. As this method requires PyArrow >= 4.0.0, the original Pandas library was used to export the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import pyspark.pandas.frame as pd\n",
    "\n",
    "rules_df = pd.DataFrame(all_rules, columns=[\"Antecedent\", \"Consequent\", \"Standardised Lift\", \"Lift\", \"Interest\", \"Confidence\"])\n",
    "rules_df.sort_values(\"Standardised Lift\", ascending=False, inplace=True)\n",
    "\n",
    "rules_df.to_string(RESULTS_DIRECTORY_PATH + \"association_rules.txt\", index=False, float_format=lambda x: f\"{x:.15f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
